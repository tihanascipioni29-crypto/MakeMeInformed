<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Page 5 — Make Me Informed</title>
<link rel="stylesheet" href="style.css">
<link rel="stylesheet" href="https://use.typekit.net/liy1hgd.css">
</head>
<body>

<div class="page">

<!-- page5.html: Article page — main content below. -->
<header class="navbar">
  <div class="logo">
    <img src="images/logo.image.jpg" alt="Logo">
    <span>www.MakeMeInformed.com</span>
  </div>

  <nav class="nav-links">
    <a href="page2.html">Articles &amp; upcoming events</a>
    <a href="page8.html">About us</a>
    <a href="page7.html">Test yourself</a>
    <a href="index.html">Home</a>
  </nav>
</header>

<main></main>

<section class="content">
  <div class="left-column">
     <h2>
        <a href="page2.html#articles">ARTICLES</a>
    </h2>
    <h1>How misinformation spreads through social networks</h1>
    <div class="article-body">
    <p class="note">How Misinformation Spreads Through Social Networks
The spread of false information online is not random. It follows identifiable patterns shaped by social networks, human psychology, and technological systems. Understanding how misinformation spreads—through virality, repost chains, bots, and emotional triggers—helps explain why false narratives can travel faster and farther than factual information.
Misinformation vs. Disinformation
False information exists in different forms. Misinformation refers to incorrect or misleading content shared without harmful intent, often by ordinary users who believe it to be true. Disinformation, on the other hand, is deliberately created and spread to deceive, manipulate opinions, or cause harm. Large-scale disinformation campaigns are often orchestrated by organized “bad actors” rather than individuals.
Bots, Sockpuppets, and Network Manipulation
A major driver of large-scale misinformation is the use of fake accounts, including bots and sockpuppets. These accounts are either synthetically created or compromised and controlled by a single entity. Their primary goal is to engineer the spread of false information and create the illusion of widespread public agreement.
Bots operate at scale and serve two main purposes. First, they rapidly spread content by reposting or retweeting it to large audiences. Second, bots often follow one another to inflate follower counts and simulate credibility and trustworthiness. Sockpuppet accounts behave more like real users: they engage in discussions, agree with one another, and challenge dissenting opinions to amplify a specific narrative.
Alarmingly, research shows that bots and sockpuppets often occupy central positions in information networks. This means they are strategically placed to maximize the visibility and reach of false information. Studies estimate that bot accounts are responsible for nearly one-fifth of all political chatter on Twitter, and that false information is more likely to be spread by bots than by real users. In some cases, bots intentionally target influential real users, who may then unknowingly reshare the misinformation to even broader audiences.
Why Humans Struggle to Detect False Information
Misinformation would be ineffective if people could easily recognize and reject it. However, numerous studies show that humans are surprisingly poor judges of deceptive content. Across experiments involving hoaxes, fake news, and fake reviews, people correctly identify false information only 53% to 78% of the time—barely better than chance in some cases.
Well-written, lengthy, and well-referenced false articles are especially deceptive. In one study, humans correctly identified hoaxes only 66% of the time, just slightly above random guessing. Even trained Wikipedia volunteers, known as “patrollers,” have mistakenly approved polished hoax articles for publication. While most hoaxes are caught quickly, around 1% of well-written hoaxes can remain undetected for over a year.
Linguistic and Emotional Triggers
Researchers have also analyzed the linguistic features of deceptive content. Fake reviews, for example, tend to be shorter than genuine ones and often rely on exaggerated language. Deceptive writing typically uses more verbs, adverbs, pronouns, and emotionally charged words. Fake negative reviews exaggerate dissatisfaction, frequently using strong emotional terms such as terrible or disappointed, while avoiding first-person pronouns like “I” to distance the writer from the sentiment.
Overall, deceptive content is often more polarized, emotional, and less readable than truthful content. These emotional cues can make misinformation more engaging and more likely to be shared.
Virality, Social Proof, and Repost Chains
Virality plays a crucial role in the spread of misinformation. People are more likely to engage with content that already appears popular, using likes, shares, and reposts as signals of credibility. This phenomenon, known as social proof, can override critical thinking and lead individuals to assume that others have already verified the information.
Research suggests that misinformation often requires virality to appear credible, whereas factual information relies on objective evidence or prior knowledge. Likes and shares can bridge this credibility gap, making false information seem trustworthy simply because it is popular. This effect was observed during the COVID-19 pandemic, when social media–driven social proof contributed to panic buying behaviors despite no real shortages.
Comment sections can also influence perception. Critical comments on highly viral posts may increase skepticism and reduce misinformation’s impact, while positive comments can increase sharing intentions—especially on posts with low initial visibility.
Echo Chambers and Repetition
Technological factors such as content personalization contribute to ideological echo chambers, where users are repeatedly exposed to the same false information across multiple channels. Repetition increases familiarity, which can make misinformation feel true over time. Combined with confirmation bias, low media literacy, and limited exposure to diverse viewpoints, this repetition makes false narratives harder to challenge.
Conclusion
Overall, misinformation spreads through social networks via a combination of technological systems, coordinated fake accounts, emotional manipulation, and human cognitive limitations. Bots and sockpuppets amplify false content, virality provides social proof, and psychological biases make users vulnerable to deception. Addressing misinformation requires not only improved platform policies, but also stronger media literacy and awareness of how online information ecosystems operate.</p>
    </div>
  </main>
  </div>
</section>

<footer>
  <p>www.MakeMeInformed.com</p>
</footer>

</div>

</body>
</html>